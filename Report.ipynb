{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296bf5e5",
   "metadata": {},
   "source": [
    "## Project 3 - Collaboration and Competition \n",
    "\n",
    "\n",
    "**Learning to play table tennis**\n",
    "\n",
    "Project Goals:\n",
    "\n",
    "* Implement the TD3 Algorithm to teach the agent to learn to play table tennis.\n",
    "* The agent is able to get a max score of 2.5 or greater\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./data_files/actor_network.png \"Actor_Network_Arch\"\n",
    "[image2]: ./data_files/critic_network.png \"Critic_Network_Arch\"\n",
    "\n",
    "\n",
    "[image3]: ./data_files/multi_agent_exp0.png \"EXP0_Multi_Agent_Graph\"\n",
    "[image4]: ./data_files/multi_agent_exp1.png \"EXP0_Multi_Agent_Graph\"\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Report\n",
    "\n",
    "\n",
    "### Learning Algorithm\n",
    "\n",
    "I've modified my project 2 implementation to solve this multi-agent learning task. The algorithm description is as follows:\n",
    "\n",
    "#### 1. Description\n",
    "I've implemented the Twin Delay DDPG (TD3) Algorithm to solve the task described above. The TD3 Algorithm is an extension of the Vanilla DDPG Algorithm that was introduced in this Nano-Degree. \n",
    "\n",
    "The TD3 differs from the original DDPG Algorithm in three distinct ways:\n",
    "\n",
    "1. TD3 learns from two separate target critic networks (Q1_target, Q2_target). Hence the \"Twin\" in Twin Delay DDPG \n",
    "\n",
    "2. The local critic network updates more frequently than the local actor network and the target networks. It is recommended to use a 2:1 update frequency ratio - i.e. update critic network 2 times for every actor / target network update. Hence the \"Delay\" in Twin Delay DDPG\n",
    "\n",
    "3. Addition of noise to target actions with the intend to stabilize the local critic network.  \n",
    "\n",
    " \n",
    "\n",
    "For further information please refer to OpenAI's Spinning Up documentation here: [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/td3.html)\n",
    "\n",
    "<u><b>Modules:</b></u>\n",
    "- <u>Replay Buffer</u>: Used to store and collect experience tuples (state, action, reward, next_state, done). \n",
    "- <u>Agent</u>: Agent class containing act, step, learn, and soft_update functions.  \n",
    "- <u>ModelsQ</u>: Definition of the Deep Neural Net Architecture using Pytorch.\n",
    "\n",
    "\n",
    "#### 2. Final Set of Hyper-parameters for EXP 1:\n",
    "```python\n",
    "#Module Variables\n",
    "#Replay Buffer\n",
    "BUFFER_SIZE = int(1e5) # memory replay buffer size\n",
    "BATCH_SIZE = 64 # batch size\n",
    "#Q Network Hyper-parameters\n",
    "GAMMA = 0.99 # Q learning discount size\n",
    "TAU = 1e-3 # for soft update from local network to taget network\n",
    "LR_ACTOR = 5e-4 # learning rate\n",
    "LR_CRITIC = 5e-4 # learning rate\n",
    "#Update frequencies\n",
    "UPDATE_CRITIC_EVERY = 1 # number of frames used to update the local network\n",
    "UPDATE_ACTOR_TARGET = 2 * UPDATE_CRITIC_EVERY\n",
    "NN_NUM_UPDATES = 2\n",
    "\n",
    "#Noise Parameters\n",
    "NOISE_SCALE = 1.0\n",
    "```\n",
    "\n",
    "#### 3. Final Model Architectures\n",
    "\n",
    "\n",
    "![alt text][image1]<center>**Figure 1**</center>\n",
    "\n",
    "\n",
    "![alt text][image2]<center>**Figure 2**</center>\n",
    "\n",
    "\n",
    "\n",
    "### Plot of Rewards\n",
    "\n",
    "\n",
    "#### Multi Agent Experiments\n",
    "\n",
    "\n",
    "#### EXP 0:\n",
    "\n",
    "This first experiment had the following noise parameter values\n",
    "\n",
    "```python\n",
    "#Please refer to line 139 inside agent.py\n",
    "#n_factor=0.4 - target actions noise factor\n",
    "actions_next = self.act(next_states, n_factor=0.4, use_target=True, add_noise=True)\n",
    "\n",
    "#Please refer to the Tennis jupyter notebook\n",
    "#n_start=0.8 - start of local actions noise factor\n",
    "train_agentTD3(agent, exp_name='EXP1',n_episodes=3000, print_every=50, max_t=1000, \n",
    "                   n_start=0.8, n_end=0.0001, n_decay=0.995):\n",
    "```\n",
    "\n",
    "\n",
    "![alt text][image3]<center>**Figure 3**</center>\n",
    "\n",
    "From the results seen above, one can see that the agent starts learning an optimal policy with smaller noise factor values ( due to noise decay ).\n",
    "\n",
    "#### EXP 1:\n",
    "\n",
    "After evaluating the results from experiment EXP0, both noise_factor parameter values were decreased (intuition).\n",
    "\n",
    "```python\n",
    "#Please refer to line 139 inside agent.py\n",
    "#n_factor=0.25 - target actions noise factor\n",
    "actions_next = self.act(next_states, n_factor=0.4, use_target=True, add_noise=True)\n",
    "\n",
    "#Please refer to the Tennis jupyter notebook\n",
    "#n_start=0.4 - start of local actions noise factor\n",
    "train_agentTD3(agent, exp_name='EXP1',n_episodes=3000, print_every=50, max_t=1000, \n",
    "                   n_start=0.4, n_end=0.0001, n_decay=0.995):\n",
    "```\n",
    "\n",
    "![alt text][image4]<center>**Figure 4**</center>\n",
    "\n",
    "With lower noise parameters values, the agent learns an optimal policy faster and in a more stable way ( as seen above ). \n",
    "\n",
    "\n",
    "\n",
    "### Ideas for Future Work\n",
    "\n",
    "#### TD3: (Same basic ideas from the last project)\n",
    "- Further explore the different combinations of hyper-parameters.\n",
    "- Implement a PPO algorithm, and compare it to this implementation.\n",
    "- Run the last experiment - EXP1 - multiple times to see if the Q function converges every time. \n",
    "- Implement more complex Q-functions.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
